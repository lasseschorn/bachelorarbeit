{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import variablen as var\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from string import ascii_uppercase\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import sys\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = var.dataset_name_1\n",
    "csv_file     = var.original_df_1\n",
    "save_dir = var.prepair_df_dir\n",
    "column = 'AMT_INCOME_TOTAL'\n",
    "result_dir = var.result_dir\n",
    "statistics_dir = var.statistics_dir\n",
    "result_file = 'result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(csv_file,filename, column, save_dir):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df.dropna() \n",
    "    y = df.loc[:, [column]].values\n",
    "    #y = df.pop(column).to_numpy()\n",
    "    df = df.drop(columns=[column])\n",
    "    X = df.loc[: , df.columns].values\n",
    "    #  X = df.to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=0)\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train, columns= df.columns)\n",
    "    X_test  = pd.DataFrame(X_test,  columns= df.columns)\n",
    "    y_train = pd.DataFrame(y_train, columns= [column])\n",
    "    y_test  = pd.DataFrame(y_test,  columns= [column])\n",
    "    \n",
    "    X_train.to_csv( save_dir  + column + '/X_train' + '.csv'  ,index=False,sep=';', decimal=',')\n",
    "    X_test.to_csv(  save_dir  + column + '/X_test'  + '.csv'  ,index=False,sep=';', decimal=',')\n",
    "    y_train.to_csv( save_dir  + column + '/y_train' + '.csv'  ,index=False,sep=';', decimal=',')\n",
    "    y_test.to_csv(  save_dir  + column + '/y_test'  + '.csv'  ,index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split_data(csv_file,filename, column, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelle https://rmisstastic.netlify.app/how-to/python/generate_html/how%20to%20generate%20missing%20values\n",
    "def ampute_mcar(X_complete, missing_rate):\n",
    "    # Mask completly at random some values\n",
    "    M = np.random.binomial(1, missing_rate, size = X_complete.shape)\n",
    "    X_obs = X_complete.copy()\n",
    "    np.putmask(X_obs, M, np.nan)\n",
    "    print('Percentage of newly generated mising values: {}'.\\\n",
    "      #Change Code:     \n",
    "      format(np.round(np.sum(pd.isna(X_obs))/X_obs.size,3)))\n",
    "    \n",
    "    # warning if a full row is missing\n",
    "    for row in X_obs:\n",
    "        if np.all(pd.isna(row)):\n",
    "            warnings.warn('Some row(s) contains only nan values.')\n",
    "            break\n",
    "\n",
    "    # warning if a full col is missing\n",
    "    for col in X_obs.T:\n",
    "        if np.all(pd.isna(col)):\n",
    "            warnings.warn('Some col(s) contains only nan values.')\n",
    "            break\n",
    "            \n",
    "    return X_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ampute(column_set):\n",
    "    for column in column_set:\n",
    "        for s in var.test_set:\n",
    "            print(column)\n",
    "            print(s)\n",
    "            X_complete = pd.read_csv(save_dir + column + '/' + s + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "            for missing_rate in var.missing_rats:\n",
    "                X_obs_mcar =  pd.DataFrame(ampute_mcar(X_complete.to_numpy(), missing_rate))\n",
    "                X_obs_mcar.columns = [X_complete.columns]\n",
    "                filename = str(np.int_(100 * missing_rate))\n",
    "                X_obs_mcar.to_csv(save_dir + column + '/' + s + '/' + filename + \".csv\" ,index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMT_INCOME_TOTAL\n",
      "y_test\n",
      "Percentage of newly generated mising values: 0.0\n",
      "Percentage of newly generated mising values: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:14: UserWarning: Some row(s) contains only nan values.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of newly generated mising values: 0.249\n",
      "Percentage of newly generated mising values: 0.502\n",
      "Percentage of newly generated mising values: 0.802\n",
      "AMT_INCOME_TOTAL\n",
      "X_test\n",
      "Percentage of newly generated mising values: 0.0\n",
      "Percentage of newly generated mising values: 0.099\n",
      "Percentage of newly generated mising values: 0.25\n",
      "Percentage of newly generated mising values: 0.5\n",
      "Percentage of newly generated mising values: 0.8\n"
     ]
    }
   ],
   "source": [
    "ampute(var.application_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def means_impute(column_set):\n",
    "\n",
    "    for column in column_set:\n",
    "        X_train = pd.read_csv(save_dir + column + '/' + var.x_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_train = pd.read_csv(save_dir + column + '/' + var.y_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_test =  pd.read_csv(save_dir + column + '/' + var.y_test  + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        \n",
    "        wert = y_train[column].mean()\n",
    "        y_files = os.listdir(save_dir + column + '/' + var.y_test + '/' )\n",
    "        results = y_test[[column]].copy()\n",
    "        for i in y_files:\n",
    "            df = pd.read_csv(save_dir + column + '/' + var.y_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            results[i[0:-4]] = df.replace(df,wert)\n",
    "        results.to_csv( result_dir  + var.methode_1 + '/' + column + '.csv'  ,index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_impute(column_set):\n",
    "\n",
    "    for column in column_set:\n",
    "        X_train = pd.read_csv(save_dir + column + '/' + var.x_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_train = pd.read_csv(save_dir + column + '/' + var.y_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_test =  pd.read_csv(save_dir + column + '/' + var.y_test  + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        \n",
    "        wert = y_train[column].mode().iloc[0,]\n",
    "        y_files = os.listdir(save_dir + column + '/' + var.y_test + '/' )\n",
    "        results = y_test[[column]].copy()\n",
    "        for i in y_files:\n",
    "            df = pd.read_csv(save_dir + column + '/' + var.y_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            results[i[0:-4]] = df.replace(df,wert)\n",
    "        results.to_csv( result_dir  + var.methode_2 + '/' + column + '.csv'  ,index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_impute(column_set):\n",
    "\n",
    "    for column in column_set:\n",
    "        X_train = pd.read_csv(save_dir + column + '/' + var.x_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_train = pd.read_csv(save_dir + column + '/' + var.y_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_test =  pd.read_csv(save_dir + column + '/' + var.y_test  + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        \n",
    "        wert = y_train[column].median()\n",
    "        y_files = os.listdir(save_dir + column + '/' + var.y_test + '/' )\n",
    "        results = y_test[[column]].copy()\n",
    "        for i in y_files:\n",
    "            df = pd.read_csv(save_dir + column + '/' + var.y_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            results[i[0:-4]] = df.replace(df,wert)\n",
    "        results.to_csv( result_dir  + var.methode_3 + '/' + column + '.csv'  ,index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df,trans_df,do_classify):\n",
    "    for column in df.columns:\n",
    "        changed = pd.Series(data=-1, index=range(df[column].size))\n",
    "        if not do_classify:\n",
    "            if type(column) != str:\n",
    "                for i, v in trans_df.iterrows():\n",
    "                    change = df[column].ne(v[column])\n",
    "                    changed = changed.where(change,1)\n",
    "                change = changed.ne(-1)\n",
    "                df[column] = df[column].where(change, -1)\n",
    "                continue\n",
    "                \n",
    "        for i, v in trans_df.iterrows():\n",
    "            change = df[column].ne(v[column])\n",
    "            df[column] = df[column].where(change, i)\n",
    "            changed = changed.where(change,1)\n",
    "        change = changed.ne(-1)\n",
    "        df[column] = df[column].where(change, -1)\n",
    "        df[column] = df[column].astype('float64')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retransform(df,trans_df, types):\n",
    "    for column in df.columns:\n",
    "        \n",
    "        for i, v in trans_df.iterrows():\n",
    "            change = df[column].ne(i)\n",
    "            df[column] = df[column].where(change,v[column])\n",
    "        change = df[column].ne(-1)\n",
    "        #df[column] = df[column].astype(types.loc[column])\n",
    "        df[column] = df[column].where(change, np.nan)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trans_table(df):\n",
    "    result = pd.DataFrame()\n",
    "    for column in df.columns:\n",
    "        s = df[[column]].sort_values(by=column, ascending=True).drop_duplicates(ignore_index=True)\n",
    "        result = pd.concat([result,s], axis=1)\n",
    "            \n",
    "        #s = df[[column]]\n",
    "        #s.to_csv(var.prepair_df_dir + col + '/columns/' + column  ,index=False,sep=';', decimal=',',  float_format='%.2f')\n",
    "        #s = pd.read_csv(var.prepair_df_dir  + col + '/columns/' + column , delimiter=\";\", decimal=\",\")\n",
    "        #result[column] = s[column]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(np.random.randint(100, size=(5, 5)), columns = list(\"ABCDE\"), \n",
    "        s = df[[column]].sort_values(by=column, ascending=True).drop_duplicates(ignore_index=True)\n",
    "        s = s.as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(dataset,column):\n",
    "    print(type(column))\n",
    "    print(column)\n",
    "    corr = dataset.corrwith(column)\n",
    "    print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df,column):\n",
    "    corr = df.corr().abs().unstack(level=0)\n",
    "    result = corr[column].drop(index=column)\n",
    "    return result.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corr[column].max(level=0))\n",
    "    kot = corr[corr>=.9]\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.heatmap(kot, cmap=\"Greens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_abs_correlations(df, n=5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_1_impute(column_set):\n",
    "    for column in column_set:\n",
    "        X_train = pd.read_csv(save_dir + column + '/' + var.x_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_train = pd.read_csv(save_dir + column + '/' + var.y_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_test =  pd.read_csv(save_dir + column + '/' + var.y_test  + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        \n",
    "        df = X_train.copy()\n",
    "        df[y_train.columns] = y_train\n",
    "        df =  df.drop(columns=['ID'])\n",
    "\n",
    "        trans_df = create_trans_table(df)\n",
    "        types = df.dtypes \n",
    "        df_temp = transform(df.copy(), trans_df )\n",
    "        #############################################\n",
    "        col_corr = correlation(df_temp ,y_train.columns[0])\n",
    "        # modi_df DF mit Ziel und Corr Spalte\n",
    "        modi_df = df_temp[[col_corr]] \n",
    "        modi_df[y_train.columns[0]] = df[y_train.columns[0]]\n",
    "        modi_df = modi_df.groupby(col_corr)[[y_train.columns[0]]].median()\n",
    "        # hinzufügen Missing Bedingung\n",
    "        df2 =pd.DataFrame([[np.nan, np.nan]], columns=[col_corr, y_train.columns[0]])\n",
    "        modi_df = modi_df.append(df2, ignore_index=True)\n",
    "        \n",
    "        y_files = os.listdir(save_dir + column + '/' + var.y_test + '/' )\n",
    "        results = y_test[[column]].copy()\n",
    "        \n",
    "        for i in y_files:\n",
    "            df_y = pd.read_csv(save_dir + column + '/' + var.y_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            df_x = pd.read_csv(save_dir + column + '/' + var.x_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "             \n",
    "            df_x = df_x.drop(columns=['ID'])  \n",
    "            x_test = transform(df_x, trans_df )\n",
    "\n",
    "            for a, v in modi_df.iterrows():\n",
    "                change = x_test[col_corr].ne(v[col_corr])\n",
    "                y_test[column] = y_test[column].where(change,v[column])\n",
    "\n",
    "            results[i[0:-4]] = retransform(y_test, trans_df, types)\n",
    "        results.to_csv( result_dir  + var.methode_4 + '/' + column + '.csv'  ,index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "correlation_1_impute(var.application_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_exception(exception: BaseException, expected: bool = True):\n",
    "    \"\"\"Prints the passed BaseException to the console, including traceback.\n",
    "\n",
    "    :param exception: The BaseException to output.\n",
    "    :param expected: Determines if BaseException was expected.\n",
    "    \"\"\"\n",
    "    output = \"[{}] {}: {}\".format('EXPECTED' if expected else 'UNEXPECTED', type(exception).__name__, exception)\n",
    "    print(output)\n",
    "    exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "    traceback.print_tb(exc_traceback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rondom_forest_classification_impute(column_set):\n",
    "    for column in column_set:\n",
    "        X_train = pd.read_csv(save_dir + column + '/' + var.x_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_train = pd.read_csv(save_dir + column + '/' + var.y_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_test =  pd.read_csv(save_dir + column + '/' + var.y_test  + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        X_train = X_train.drop(columns=['ID'])\n",
    "        df = X_train.copy()\n",
    "        df[y_train.columns] = y_train\n",
    "        trans_df = create_trans_table(df)\n",
    "        types = df.dtypes \n",
    "        x_temp = transform(X_train.copy(), trans_df, True )\n",
    "        y_temp = transform(y_train.copy(), trans_df, True )\n",
    "        ##############################################\n",
    "        sc = StandardScaler()\n",
    "        x_temp = sc.fit_transform(x_temp)\n",
    "        x_temp_test = sc.transform(x_temp)\n",
    "        #############################################\n",
    "        best_tree = -1\n",
    "        best_mean_squared_error = -1\n",
    "        ##############################################\n",
    "        for i in [40,50,60,70,80,90,100,110,120,130,140,150,160,170]:\n",
    "            try:\n",
    "                classifier = RandomForestClassifier(n_estimators=i, random_state=0)\n",
    "                classifier.fit(x_temp, y_temp)\n",
    "            #except Exception as exception:\n",
    "            except MemoryError as error:\n",
    "                log_exception(error)\n",
    "                continue\n",
    "            #    print(str(i) + ' kann nicht genommen werden. Speicherporbelme')\n",
    "            ###    continue                \n",
    "            y_pred = classifier.predict(x_temp_test)\n",
    "            ###################################\n",
    "            new_mean_squared_error = metrics.mean_squared_error(y_temp, y_pred)\n",
    "            print(str(i) + \" durch\")\n",
    "            if best_tree == -1 or new_mean_squared_error < best_mean_squared_error:\n",
    "                best_mean_squared_error = new_mean_squared_error\n",
    "                best_tree = i \n",
    "                print(best_tree)\n",
    "                print(best_mean_squared_error)\n",
    "                print(\"Best Tree \" + str(best_tree))\n",
    "        \n",
    "        classifier = RandomForestClassifier(n_estimators=best_tree, random_state=0)\n",
    "        classifier.fit(x_temp, y_temp)\n",
    "        ######################################\n",
    "        ######################################\n",
    "        results = y_test[[column]].copy()\n",
    "        y_files = os.listdir(save_dir + column + '/' + var.y_test + '/' )\n",
    "        for i in y_files:\n",
    "            df_y = pd.read_csv(save_dir + column + '/' + var.y_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            df_x = pd.read_csv(save_dir + column + '/' + var.x_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            \n",
    "            df_x = df_x.drop(columns=['ID'])\n",
    "            x_test = transform(df_x, trans_df, True )\n",
    "            X_test = sc.transform(x_test)\n",
    "            y_pred =  classifier.predict(X_test)\n",
    "            \n",
    "            y_test = pd.DataFrame(data=y_pred, columns=[df_y.columns[0]])\n",
    "            results[i[0:-4]] = retransform(y_test, trans_df, types)\n",
    "        results.to_csv( result_dir  + var.methode_7 + '/' + column + '.csv'  ,index=False,sep=';', decimal=',')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:24: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 durch\n",
      "40\n",
      "16109.520305569245\n",
      "Best Tree 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:24: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 durch\n",
      "50\n",
      "15348.134631181207\n",
      "Best Tree 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:24: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 durch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:24: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 durch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:24: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 durch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:24: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXPECTED] MemoryError: could not allocate 231211008 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-28-2795cb9ffcb2>\", line 24, in rondom_forest_classification_impute\n",
      "    classifier.fit(x_temp, y_temp)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 383, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1007, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 835, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 754, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 209, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 590, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 165, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 877, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 367, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 146, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 244, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 739, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 711, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 41, in sklearn.tree._utils.safe_realloc\n",
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:24: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXPECTED] MemoryError: could not allocate 231211008 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-28-2795cb9ffcb2>\", line 24, in rondom_forest_classification_impute\n",
      "    classifier.fit(x_temp, y_temp)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 383, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1007, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 835, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 754, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 209, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 590, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 165, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 877, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 367, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 146, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 244, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 739, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 711, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 41, in sklearn.tree._utils.safe_realloc\n",
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:24: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXPECTED] MemoryError: could not allocate 231211008 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-28-2795cb9ffcb2>\", line 24, in rondom_forest_classification_impute\n",
      "    classifier.fit(x_temp, y_temp)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 383, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1007, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 835, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 754, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 209, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 590, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 165, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 877, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 367, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 146, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 244, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 739, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 711, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 41, in sklearn.tree._utils.safe_realloc\n",
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:24: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXPECTED] MemoryError: could not allocate 231211008 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-28-2795cb9ffcb2>\", line 24, in rondom_forest_classification_impute\n",
      "    classifier.fit(x_temp, y_temp)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 383, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1007, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 835, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 754, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 209, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 590, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 165, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 877, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 367, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 146, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 244, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 739, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 711, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 41, in sklearn.tree._utils.safe_realloc\n",
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:24: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXPECTED] MemoryError: could not allocate 231211008 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-28-2795cb9ffcb2>\", line 24, in rondom_forest_classification_impute\n",
      "    classifier.fit(x_temp, y_temp)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 383, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1007, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 835, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 754, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 209, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 590, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 165, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 877, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 367, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 146, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 244, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 739, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 711, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 41, in sklearn.tree._utils.safe_realloc\n",
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:24: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXPECTED] MemoryError: could not allocate 231211008 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-28-2795cb9ffcb2>\", line 24, in rondom_forest_classification_impute\n",
      "    classifier.fit(x_temp, y_temp)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 383, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1007, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 835, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 754, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 209, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 590, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 165, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 877, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 367, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 146, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 244, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 739, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 711, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 41, in sklearn.tree._utils.safe_realloc\n",
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:24: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXPECTED] MemoryError: could not allocate 231211008 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-28-2795cb9ffcb2>\", line 24, in rondom_forest_classification_impute\n",
      "    classifier.fit(x_temp, y_temp)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 383, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1007, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 835, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 754, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 209, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 590, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 165, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 877, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 367, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 146, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 244, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 739, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 711, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 41, in sklearn.tree._utils.safe_realloc\n",
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:24: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXPECTED] MemoryError: could not allocate 231211008 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-28-2795cb9ffcb2>\", line 24, in rondom_forest_classification_impute\n",
      "    classifier.fit(x_temp, y_temp)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 383, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1007, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 835, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 754, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 209, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 590, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 165, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 877, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 367, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 146, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 244, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 739, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 711, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 41, in sklearn.tree._utils.safe_realloc\n",
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:24: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXPECTED] MemoryError: could not allocate 231211008 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-28-2795cb9ffcb2>\", line 24, in rondom_forest_classification_impute\n",
      "    classifier.fit(x_temp, y_temp)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 383, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1007, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 835, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 754, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 209, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 590, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 165, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 877, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\GreenBook\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 367, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 146, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 244, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 739, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 711, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 41, in sklearn.tree._utils.safe_realloc\n",
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:43: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "rondom_forest_classification_impute(var.application_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rondom_forest_regression_impute(column_set):\n",
    "    for column in column_set:\n",
    "        X_train = pd.read_csv(save_dir + column + '/' + var.x_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_train = pd.read_csv(save_dir + column + '/' + var.y_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_test =  pd.read_csv(save_dir + column + '/' + var.y_test  + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        \n",
    "        X_train = X_train.drop(columns=['ID'])\n",
    "        trans_df = create_trans_table(X_train)\n",
    "        X_train = transform(X_train.copy(), trans_df, False )\n",
    "        ##############################################\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_train_test = sc.transform(X_train)\n",
    "        #############################################\n",
    "        best_tree = -1\n",
    "        best_mean_squared_error = -1\n",
    "        ##############################################\n",
    "        for i in [40,50,60,70,80,90,100,110,120,130,140,150,160,170]:\n",
    "        #for i in [110]:\n",
    "            ###################################\n",
    "            regressor = RandomForestRegressor(n_estimators=i, random_state=0)\n",
    "            regressor.fit(X_train, y_train)\n",
    "            y_pred = regressor.predict(X_train_test)\n",
    "            ###################################\n",
    "            new_mean_squared_error = metrics.mean_squared_error(y_train, y_pred)\n",
    "            print(str(i) + \" durch\")\n",
    "            if best_tree == -1 or new_mean_squared_error < best_mean_squared_error:\n",
    "                best_mean_squared_error = new_mean_squared_error\n",
    "                best_tree = i \n",
    "                print(best_tree)\n",
    "                print(best_mean_squared_error)\n",
    "                print(\"Best Tree \" + str(best_tree))\n",
    "        \n",
    "        regressor = RandomForestRegressor(n_estimators=best_tree, random_state=0)\n",
    "        regressor.fit(X_train, y_train)\n",
    "        ######################################\n",
    "        ######################################\n",
    "        results = y_test[[column]].copy()\n",
    "        y_files = os.listdir(save_dir + column + '/' + var.y_test + '/' )\n",
    "        for i in y_files:\n",
    "            df_y = pd.read_csv(save_dir + column + '/' + var.y_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            df_x = pd.read_csv(save_dir + column + '/' + var.x_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            \n",
    "            df_x = df_x.drop(columns=['ID'])\n",
    "            df_x = transform(df_x.copy(), trans_df, False )\n",
    "            print(i[0:-4])\n",
    "            X_test = sc.transform(df_x)\n",
    "            y_pred =  regressor.predict(X_test)\n",
    "            results[i[0:-4]] = y_pred\n",
    "        results.to_csv( result_dir  + var.methode_8 + '/' + column + '.csv'  ,index=False,sep=';', decimal=',')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:22: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 durch\n",
      "40\n",
      "62210350098.00211\n",
      "Best Tree 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:22: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 durch\n",
      "50\n",
      "56579828345.17495\n",
      "Best Tree 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:22: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 durch\n",
      "60\n",
      "54902268112.34865\n",
      "Best Tree 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:22: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 durch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:22: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 durch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:22: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 durch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:22: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 durch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:22: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 durch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:22: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 durch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:22: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 durch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:22: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 durch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:22: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 durch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:22: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 durch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:22: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170 durch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:35: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "25\n",
      "50\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "rondom_forest_regression_impute(var.application_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        s = trans_df[col_corr]\n",
    "        for i, v in s.items():\n",
    "            new_df = df[df[col_corr] == i ]\n",
    "            mod = new_df[y_train.columns[0]].mode().iloc[0]\n",
    "            df2 =pd.DataFrame([[i, mod]], columns=list('IM'))\n",
    "            modi_df = modi_df.append(df2)\n",
    "        df2 =pd.DataFrame([[i, mod]], columns=list('IM'))\n",
    "        y_train.columns[0]\n",
    "            \n",
    "            \n",
    "        #Mode gibt eine Serie's wieder\n",
    "        #print(new_df.head())\n",
    "        if not new_df[column].mode().empty:\n",
    "            new_df[column].fillna(new_df[column].mode().iloc[0], inplace=True)       \n",
    "        df.update(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lern_model(train_path, dep_var,cat_names,cont_names, X_train,X_train_length, epoche=5 ):\n",
    "\n",
    "    procs = [FillMissing, Categorify, Normalize]\n",
    "    data = (TabularList.from_df(df, path=train_path, cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "                            .split_by_idx(valid_idx=range(1,X_train_length))\n",
    "                           #.split_none()\n",
    "                            .label_from_df(cols=dep_var)\n",
    "                            .databunch())\n",
    "    learn = tabular_learner(data, layers=[200,100], metrics=accuracy)\n",
    "    learn.fit(epoche, 1e-2)\n",
    "    learn.save('mini_train')\n",
    "    learn.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuronal_network_impute(column_set):\n",
    "    for column in column_set:\n",
    "        X_train = pd.read_csv(save_dir + column + '/' + var.x_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_train = pd.read_csv(save_dir + column + '/' + var.y_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_test =  pd.read_csv(save_dir + column + '/' + var.y_test  + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        \n",
    "        df_y = pd.read_csv(save_dir + column + '/' + var.y_test + '/' + '0.csv' , delimiter=\";\", decimal=\",\")\n",
    "        df_x = pd.read_csv(save_dir + column + '/' + var.x_test + '/' + '0.csv' , delimiter=\";\", decimal=\",\")\n",
    "        ############\n",
    "        ############\n",
    "        #RANNBATSCHEN\n",
    "        ########\n",
    "        #########\n",
    "        X_train_length =  X_train.shape[0]\n",
    "        X_train = X_train.drop(columns=['ID'])\n",
    "        df_x = df_x.drop(columns=['ID'])\n",
    "        \n",
    "        X_train = X_train.append(df_x, ignore_index=True)\n",
    "        trans_df = create_trans_table(X_train)\n",
    "        df = transform(X_train.copy(), trans_df, True )\n",
    "        y_train = y_train.append(df_y, ignore_index=True)\n",
    "        df[y_train.columns] = y_train\n",
    "        print(X_train_length)\n",
    "        print(df.shape[0])\n",
    "        train_path = save_dir + column + '/train/'  \n",
    "        df.to_csv( train_path + 'train' + '.csv' ,index=False,sep=';', decimal=',')\n",
    "        ##############################################\n",
    "        lern_model(train_path, column,X_train.columns,[column], df,X_train_length -1 , epoche=5 )\n",
    "        ######################################\n",
    "        ######################################\n",
    "        \n",
    "        #results = y_test[[column]].copy()\n",
    "        results = pd.DataFrame()\n",
    "        y_files = os.listdir(save_dir + column + '/' + var.y_test + '/' )\n",
    "        for i in y_files:\n",
    "            df_y = pd.read_csv(save_dir + column + '/' + var.y_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            df_x = pd.read_csv(save_dir + column + '/' + var.x_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "\n",
    "            \n",
    "            df_x = df_x.drop(columns=['ID'])\n",
    "            df_test = transform(df_x.copy(), trans_df, False )\n",
    "            df_y = df_y.fillnan(-1)\n",
    "            df_test[df_y.columns] = df_y\n",
    "            \n",
    "            data = (TabularList.from_df(df_test, path=train_path, cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "                           #.split_by_idx(valid_idx=range(800,1000))\n",
    "                           .split_none()\n",
    "                            .label_from_df(cols=dep_var)\n",
    "                           .databunch())\n",
    "            \n",
    "            learn = load_learner(train_path)\n",
    "            learn.validate(data.dl(DatasetType.Train),metric=mean_absolute_error)\n",
    "            \n",
    "            results[i[0:-4]] = learn.validate(data.dl(DatasetType.Train),metric=mean_absolute_error)\n",
    "        results.to_csv( result_dir  + var.methode_9 + '/' + column + '.csv'  ,index=False,sep=';', decimal=',')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuronal_network_impute(var.application_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\.fastai\\data\\adult_sample\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.337365</td>\n",
       "      <td>0.353097</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>01:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Category tensor(1), tensor(1), tensor([0.4217, 0.5783]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.tabular import *\n",
    "adult = untar_data(URLs.ADULT_SAMPLE)\n",
    "#adult = \"C:\\Users\\GreenBook\\.fastai\\data\\adult_sample\"\n",
    "print(adult)\n",
    "df = pd.read_csv(adult/'adult.csv')\n",
    "dep_var = 'salary'\n",
    "cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "cont_names = ['education-num', 'hours-per-week', 'age', 'capital-loss', 'fnlwgt', 'capital-gain']\n",
    "procs = [FillMissing, Categorify, Normalize]\n",
    "data = (TabularList.from_df(df, path=adult, cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "        .split_by_idx(valid_idx=range(800,1000))\n",
    "        .label_from_df(cols=dep_var)\n",
    "        .databunch())\n",
    "        \n",
    "learn = tabular_learner(data, layers=[200,100], metrics=accuracy)\n",
    "learn.fit(1, 1e-2)\n",
    "learn.save('mini_train')\n",
    "learn.export()\n",
    "learn = load_learner(adult)\n",
    "learn.predict(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for methode in var.methodes:\n",
    "    files = os.listdir(result_dir + methode+ '/' )\n",
    "    for file in files:\n",
    "        org_column = file[0:-4]\n",
    "        result = pd.read_csv(result_dir + methode+ '/' +  file , delimiter=\";\", decimal=\",\")\n",
    "        if os.path.exists(statistics_dir + 'statistics.csv'):\n",
    "            statistics = pd.read_csv(statistics_dir + 'statistics.csv' , delimiter=\";\", decimal=\",\")\n",
    "        else:\n",
    "            statistics = pd.DataFrame()\n",
    "        statistic = pd.DataFrame()\n",
    "        original = result.loc[: , org_column]\n",
    "        result = result.drop(columns=[org_column])\n",
    "        columns = np.array(['methode','type','column', 'misssing', 'Wert'])\n",
    "\n",
    "        \n",
    "        for column in result.columns:\n",
    "            values = np.array([methode,'mean_absolute_error', org_column])\n",
    "            values = np.append(values, [column])\n",
    "            values = np.append(values , [metrics.mean_absolute_error(original, result[column])])\n",
    "            statistics = pd.DataFrame(np.array([values]), columns=columns).append(statistics, ignore_index=True)\n",
    "\n",
    "        statistics.to_csv(statistics_dir + 'statistics.csv', index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for methode in var.methodes:\n",
    "    files = os.listdir(result_dir + methode+ '/' )\n",
    "    for file in files:\n",
    "        org_column = file[0:-4]\n",
    "        result = pd.read_csv(result_dir + methode+ '/' +  file , delimiter=\";\", decimal=\",\")\n",
    "        if os.path.exists(statistics_dir + 'statistics.csv'):\n",
    "            statistics = pd.read_csv(statistics_dir + 'statistics.csv' , delimiter=\";\", decimal=\",\")\n",
    "\n",
    "        else:\n",
    "            statistics = pd.DataFrame()\n",
    "            \n",
    "        original = result.loc[: , org_column]\n",
    "        result = result.drop(columns=[org_column])\n",
    "        columns = np.array(['methode','type','column', 'misssing', 'Wert'])\n",
    "\n",
    "        \n",
    "\n",
    "        for column in result.columns:\n",
    "            values = np.array([methode,'mean_squared_error', org_column])\n",
    "            values = np.append(values, [column])\n",
    "            values = np.append(values , [metrics.mean_squared_error(original, result[column])])\n",
    "            statistics = pd.DataFrame(np.array([values]), columns=columns).append(statistics, ignore_index=True)\n",
    "\n",
    "        statistics.to_csv(statistics_dir + 'statistics.csv', index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for methode in var.methodes:\n",
    "    files = os.listdir(result_dir + methode+ '/' )\n",
    "    for file in files:\n",
    "        org_column = file[0:-4]\n",
    "        result = pd.read_csv(result_dir + methode+ '/' +  file , delimiter=\";\", decimal=\",\")\n",
    "        if os.path.exists(statistics_dir + 'statistics.csv'):\n",
    "            statistics = pd.read_csv(statistics_dir + 'statistics.csv' , delimiter=\";\", decimal=\",\")\n",
    "        else:\n",
    "            statistics = pd.DataFrame()\n",
    "            \n",
    "        original = result.loc[: , org_column]\n",
    "        result = result.drop(columns=[org_column])\n",
    "        columns = np.array(['methode','type','column', 'misssing', 'Wert'])\n",
    "\n",
    "\n",
    "        for column in result.columns:\n",
    "            values = np.array([methode,'root_mean_squared_error', org_column])\n",
    "            values = np.append(values, [column])\n",
    "            values = np.append(values , [np.sqrt(metrics.mean_squared_error(original, result[column]))])\n",
    "            statistics = pd.DataFrame(np.array([values]), columns=columns).append(statistics, ignore_index=True)\n",
    "        \n",
    "\n",
    "        statistics.to_csv(statistics_dir + 'statistics.csv', index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D   E\n",
      "0  58  67  94  16  29\n",
      "1  75  67  53  50  69\n",
      "2   1  48  99  73  49\n",
      "3  63  64  53   5  87\n",
      "4   0  54  34  83  18\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.randint(100, size=(5, 5)), columns = list(\"ABCDE\"), \n",
    "                  index = [i for i in range(5)])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    int32\n",
      "B    int32\n",
      "C    int32\n",
      "D    int32\n",
      "E    int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D   E\n",
      "0  40  18   7   9  18\n",
      "1  62  21  35  21  20\n",
      "2  68  73  37  23  33\n",
      "3  74  88  54  51  45\n",
      "4  97  97  95  87  52\n"
     ]
    }
   ],
   "source": [
    "trans_df = create_trans_table(df)\n",
    "print(trans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B     C    D    E\n",
      "0   97   18  54.0   23   45\n",
      "1   68   21  35.0    9   52\n",
      "2   40   73   7.0   87   20\n",
      "3   62   88  37.0   51   33\n",
      "4   74   97  95.0   21   18\n",
      "5  101  102   NaN  104  105\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.DataFrame([[101, 102,np.nan,104,105]], columns=list('ABCDE'))\n",
    "df = df.append(df2, ignore_index=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C    D    E\n",
      "0  4.0  0.0  3.0  2.0  3.0\n",
      "1  2.0  1.0  1.0  0.0  4.0\n",
      "2  0.0  2.0  0.0  4.0  1.0\n",
      "3  1.0  3.0  2.0  3.0  2.0\n",
      "4  3.0  4.0  4.0  1.0  0.0\n",
      "5 -1.0 -1.0 -1.0 -1.0 -1.0\n"
     ]
    }
   ],
   "source": [
    "df1 = transform(df.copy(), trans_df )\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      A     B     C     D     E\n",
      "0  97.0  18.0  54.0  23.0  45.0\n",
      "1  68.0  21.0  35.0   9.0  52.0\n",
      "2  40.0  73.0   7.0  87.0  20.0\n",
      "3  62.0  88.0  37.0  51.0  33.0\n",
      "4  74.0  97.0  95.0  21.0  18.0\n",
      "5   NaN   NaN   NaN   NaN   NaN\n"
     ]
    }
   ],
   "source": [
    "df2 = retransform(df1.copy(), trans_df, df.dtypes)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    object\n",
      "B    object\n",
      "C    object\n",
      "D    object\n",
      "E    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df2.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    object\n",
      "B    object\n",
      "C    object\n",
      "D    object\n",
      "E    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df2.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A      int64\n",
      "B      int64\n",
      "C    float64\n",
      "D      int64\n",
      "E      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -1\n",
      "1   -1\n",
      "2   -1\n",
      "3   -1\n",
      "4   -1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "changed = pd.Series(data=-1, index=range(df['A'].size))\n",
    "print(changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     True\n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4     True\n",
      "5    False\n",
      "6    False\n",
      "7    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "v = pd.Series(range(8))\n",
    "\n",
    "\n",
    "change = df['C'].eq(v)\n",
    "print(change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B  C  D    E\n",
      "0  2  1  5  4  2.0\n",
      "1  3  3  1  1  1.0\n",
      "2  4  2  5  3  3.0\n",
      "3  0  0  3  2  4.0\n",
      "4  1  4  4  0  0.0\n"
     ]
    }
   ],
   "source": [
    "df['C'] = df['C'].where(change, \"5\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C    D    E  0\n",
      "0  3.0  0.0  0.0  2.0  1.0  0\n",
      "1  0.0  3.0  3.0  0.0  3.0  1\n",
      "2  1.0  1.0  1.0  4.0  2.0  2\n",
      "3  2.0  4.0  2.0  1.0  0.0  3\n",
      "4  4.0  2.0  4.0  3.0  4.0  4\n",
      "5  NaN  NaN  NaN  NaN  NaN  5\n",
      "6  NaN  NaN  NaN  NaN  NaN  6\n",
      "7  NaN  NaN  NaN  NaN  NaN  7\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df,v], axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['A', 'B', 'C', 'D', 'E'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1   -1\n",
      "2    1\n",
      "3    1\n",
      "4   -1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "changed = changed.where(change, 1)\n",
    "print(changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = changed.eq(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float              float64\n",
      "int                  int64\n",
      "datetime    datetime64[ns]\n",
      "string              object\n",
      "dtype: object\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'float': [1.0],\n",
    "                   'int': [1],\n",
    "                   'datetime': [pd.Timestamp('20180310')],\n",
    "                   'string': ['foo']})\n",
    "types = df.dtypes\n",
    "print(types.head())\n",
    "print(types.loc['int'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0     1   2  3\n",
      "0    <NA> NaN  3\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([[\"\", pd.NA, np.nan, \"3\"]])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     object\n",
      "1     object\n",
      "2    float64\n",
      "3     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   float  int   datetime string\n",
      "0    1.0    1 2018-03-10    foo\n",
      "float                int64\n",
      "int                  int64\n",
      "datetime    datetime64[ns]\n",
      "string              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df =  pd.DataFrame({'float': [1.0],\n",
    "                   'int': [1],\n",
    "                   'datetime': [pd.Timestamp('20180310')],\n",
    "                   'string': ['foo']})\n",
    "df1 = pd.DataFrame(data=[2,3,4,5,6], columns=['a'])\n",
    "print(df)\n",
    "df['float'] = df1['a']\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[3].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://files.fast.ai/data/examples/mnist_tiny.tgz\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to C:\\Users\\GreenBook/.cache\\torch\\hub\\checkpoints\\resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2daef2d13b2442f4ac1caf22c0a33caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\GreenBook\\\\.fastai\\\\data\\\\mnist_tiny\\\\models\\\\mini_train.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-96c6ecac6dc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mdatabunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         .normalize(imagenet_stats))\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mlearn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn_learner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mini_train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mlearn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_learner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, file, device, strict, with_opt, purge, remove_module)\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;34mf'{file}.pth'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_pathlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[0mdistrib_barrier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'opt'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m             \u001b[0mmodel_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\GreenBook\\\\.fastai\\\\data\\\\mnist_tiny\\\\models\\\\mini_train.pth'"
     ]
    }
   ],
   "source": [
    "from fastai.vision import *\n",
    "mnist = untar_data(URLs.MNIST_TINY)\n",
    "tfms = get_transforms(do_flip=False)\n",
    "data = (ImageList.from_folder(mnist)\n",
    "        .split_by_folder()          \n",
    "        .label_from_folder()\n",
    "        .add_test_folder('test')\n",
    "        .transform(tfms, size=32)\n",
    "        .databunch()\n",
    "        .normalize(imagenet_stats)) \n",
    "\n",
    "learn = cnn_learner(data, models.resnet18).load('mini_train')\n",
    "learn.export()\n",
    "learn = load_learner(mnist)\n",
    "img = data.train_ds[0][0]\n",
    "learn.predict(img)\n",
    "learn = load_learner(mnist, test=ImageList.from_folder(mnist/'test'))\n",
    "preds,y = learn.get_preds(ds_type=DatasetType.Test)\n",
    "preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
