{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import variablen as var\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from string import ascii_uppercase\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import sys\n",
    "import traceback\n",
    "from fastai.tabular import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = var.dataset_name_1\n",
    "csv_file     = var.original_df_1\n",
    "save_dir = var.prepair_df_dir\n",
    "column = 'AMT_INCOME_TOTAL'\n",
    "result_dir = var.result_dir\n",
    "statistics_dir = var.statistics_dir\n",
    "result_file = 'result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(csv_file,filename, column, save_dir):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df.dropna() \n",
    "    y = df.loc[:, [column]].values\n",
    "    #y = df.pop(column).to_numpy()\n",
    "    df = df.drop(columns=[column])\n",
    "    X = df.loc[: , df.columns].values\n",
    "    #  X = df.to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=0)\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train, columns= df.columns)\n",
    "    X_test  = pd.DataFrame(X_test,  columns= df.columns)\n",
    "    y_train = pd.DataFrame(y_train, columns= [column])\n",
    "    y_test  = pd.DataFrame(y_test,  columns= [column])\n",
    "    \n",
    "    X_train.to_csv( save_dir  + column + '/X_train' + '.csv'  ,index=False,sep=';', decimal=',')\n",
    "    X_test.to_csv(  save_dir  + column + '/X_test'  + '.csv'  ,index=False,sep=';', decimal=',')\n",
    "    y_train.to_csv( save_dir  + column + '/y_train' + '.csv'  ,index=False,sep=';', decimal=',')\n",
    "    y_test.to_csv(  save_dir  + column + '/y_test'  + '.csv'  ,index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelle https://rmisstastic.netlify.app/how-to/python/generate_html/how%20to%20generate%20missing%20values\n",
    "def ampute_mcar(X_complete, missing_rate):\n",
    "    # Mask completly at random some values\n",
    "    M = np.random.binomial(1, missing_rate, size = X_complete.shape)\n",
    "    X_obs = X_complete.copy()\n",
    "    np.putmask(X_obs, M, np.nan)\n",
    "    print('Percentage of newly generated mising values: {}'.\\\n",
    "      #Change Code:     \n",
    "      format(np.round(np.sum(pd.isna(X_obs))/X_obs.size,3)))\n",
    "    \n",
    "    # warning if a full row is missing\n",
    "    for row in X_obs:\n",
    "        if np.all(pd.isna(row)):\n",
    "            warnings.warn('Some row(s) contains only nan values.')\n",
    "            break\n",
    "\n",
    "    # warning if a full col is missing\n",
    "    for col in X_obs.T:\n",
    "        if np.all(pd.isna(col)):\n",
    "            warnings.warn('Some col(s) contains only nan values.')\n",
    "            break\n",
    "            \n",
    "    return X_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ampute(column_set):\n",
    "    for column in column_set:\n",
    "        for s in var.test_set:\n",
    "            print(column)\n",
    "            print(s)\n",
    "            X_complete = pd.read_csv(save_dir + column + '/' + s + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "            for missing_rate in var.missing_rats:\n",
    "                X_obs_mcar =  pd.DataFrame(ampute_mcar(X_complete.to_numpy(), missing_rate))\n",
    "                X_obs_mcar.columns = [X_complete.columns]\n",
    "                filename = str(np.int_(100 * missing_rate))\n",
    "                X_obs_mcar.to_csv(save_dir + column + '/' + s + '/' + filename + \".csv\" ,index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_exception(exception: BaseException, expected: bool = True):\n",
    "    \"\"\"Prints the passed BaseException to the console, including traceback.\n",
    "\n",
    "    :param exception: The BaseException to output.\n",
    "    :param expected: Determines if BaseException was expected.\n",
    "    \"\"\"\n",
    "    output = \"[{}] {}: {}\".format('EXPECTED' if expected else 'UNEXPECTED', type(exception).__name__, exception)\n",
    "    print(output)\n",
    "    exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "    traceback.print_tb(exc_traceback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trans_table(df):\n",
    "    result = pd.DataFrame()\n",
    "    for column in df.columns:\n",
    "        s = df[[column]].sort_values(by=column, ascending=True).drop_duplicates(ignore_index=True)\n",
    "        result = pd.concat([result,s], axis=1)\n",
    "            \n",
    "        #s = df[[column]]\n",
    "        #s.to_csv(var.prepair_df_dir + col + '/columns/' + column  ,index=False,sep=';', decimal=',',  float_format='%.2f')\n",
    "        #s = pd.read_csv(var.prepair_df_dir  + col + '/columns/' + column , delimiter=\";\", decimal=\",\")\n",
    "        #result[column] = s[column]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df,trans_df,do_classify):\n",
    "    for column in df.columns:\n",
    "        changed = pd.Series(data=-1, index=range(df[column].size))\n",
    "        if not do_classify:\n",
    "            if type(column) != str:\n",
    "                for i, v in trans_df.iterrows():\n",
    "                    change = df[column].ne(v[column])\n",
    "                    changed = changed.where(change,1)\n",
    "                change = changed.ne(-1)\n",
    "                df[column] = df[column].where(change, -1)\n",
    "                continue\n",
    "                \n",
    "        for i, v in trans_df.iterrows():\n",
    "            change = df[column].ne(v[column])\n",
    "            df[column] = df[column].where(change, i)\n",
    "            changed = changed.where(change,1)\n",
    "        change = changed.ne(-1)\n",
    "        df[column] = df[column].where(change, -1)\n",
    "        df[column] = df[column].astype('float64')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retransform(df,trans_df, types):\n",
    "    for column in df.columns:\n",
    "        \n",
    "        for i, v in trans_df.iterrows():\n",
    "            change = df[column].ne(i)\n",
    "            df[column] = df[column].where(change,v[column])\n",
    "        change = df[column].ne(-1)\n",
    "        #df[column] = df[column].astype(types.loc[column])\n",
    "        df[column] = df[column].where(change, np.nan)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_pearson(df,column):\n",
    "    corr = df.corr(method='pearson').abs().unstack(level=0)\n",
    "    result = corr[column].drop(index=column)\n",
    "    return result.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_kendall(df,column):\n",
    "    corr = df.corr(method='kendall').abs().unstack(level=0)\n",
    "    result = corr[column].drop(index=column)\n",
    "    return result.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_spearman(df,column):\n",
    "    corr = df.corr(method='spearman').abs().unstack(level=0)\n",
    "    result = corr[column].drop(index=column)\n",
    "    return result.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lern_model(train_path, dep_var,cat_names,cont_names, df,df_length,df_ende,procs, epoche=5 ):\n",
    "    data = (TabularList.from_df(df, path=train_path, cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "                            .split_by_idx(valid_idx=list(range(df_length,df_ende )))\n",
    "                            .label_from_df(cols=dep_var)\n",
    "                            .databunch(bs=1024))\n",
    "    learn = tabular_learner(data, layers=[500,200], metrics=root_mean_squared_error)\n",
    "    learn.fit(epoche, 4e-2)\n",
    "    learn.save('mini_train')\n",
    "    learn.export()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def means_impute(column_set):\n",
    "\n",
    "    for column in column_set:\n",
    "        X_train = pd.read_csv(save_dir + column + '/' + var.x_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_train = pd.read_csv(save_dir + column + '/' + var.y_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_test =  pd.read_csv(save_dir + column + '/' + var.y_test  + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        \n",
    "        wert = y_train[column].mean()\n",
    "        y_files = os.listdir(save_dir + column + '/' + var.y_test + '/' )\n",
    "        results = y_test[[column]].copy()\n",
    "        for i in y_files:\n",
    "            df = pd.read_csv(save_dir + column + '/' + var.y_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            results[i[0:-4]] = df.replace(df,wert)\n",
    "        results.to_csv( result_dir  + var.methode_1 + '/' + column + '.csv'  ,index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_impute(column_set):\n",
    "\n",
    "    for column in column_set:\n",
    "        X_train = pd.read_csv(save_dir + column + '/' + var.x_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_train = pd.read_csv(save_dir + column + '/' + var.y_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_test =  pd.read_csv(save_dir + column + '/' + var.y_test  + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        \n",
    "        wert = y_train[column].mode().iloc[0,]\n",
    "        y_files = os.listdir(save_dir + column + '/' + var.y_test + '/' )\n",
    "        results = y_test[[column]].copy()\n",
    "        for i in y_files:\n",
    "            df = pd.read_csv(save_dir + column + '/' + var.y_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            results[i[0:-4]] = df.replace(df,wert)\n",
    "        results.to_csv( result_dir  + var.methode_2 + '/' + column + '.csv'  ,index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_impute(column_set):\n",
    "\n",
    "    for column in column_set:\n",
    "        X_train = pd.read_csv(save_dir + column + '/' + var.x_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_train = pd.read_csv(save_dir + column + '/' + var.y_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_test =  pd.read_csv(save_dir + column + '/' + var.y_test  + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        \n",
    "        wert = y_train[column].median()\n",
    "        y_files = os.listdir(save_dir + column + '/' + var.y_test + '/' )\n",
    "        results = y_test[[column]].copy()\n",
    "        for i in y_files:\n",
    "            df = pd.read_csv(save_dir + column + '/' + var.y_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            results[i[0:-4]] = df.replace(df,wert)\n",
    "        results.to_csv( result_dir  + var.methode_3 + '/' + column + '.csv'  ,index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_1_impute(column_set):\n",
    "    for column in column_set:\n",
    "        X_train = pd.read_csv(save_dir + column + '/' + var.x_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_train = pd.read_csv(save_dir + column + '/' + var.y_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_test =  pd.read_csv(save_dir + column + '/' + var.y_test  + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        \n",
    "        df = X_train.copy()\n",
    "        df[y_train.columns] = y_train\n",
    "        df =  df.drop(columns=['ID'])\n",
    "\n",
    "        trans_df = create_trans_table(df)\n",
    "        types = df.dtypes \n",
    "        df_temp = transform(df.copy(), trans_df,True )\n",
    "        #############################################\n",
    "        col_corr = correlation_pearson(df_temp ,y_train.columns[0])\n",
    "        # modi_df DF mit Ziel und Corr Spalte\n",
    "        modi_df = df_temp[[col_corr]] \n",
    "        modi_df[y_train.columns[0]] = df_temp[y_train.columns[0]]\n",
    "        modi_df = modi_df.groupby([col_corr])[y_train.columns[0]].median()\n",
    "        modi_df =  modi_df.reset_index(level=col_corr)\n",
    "        # hinzufügen Missing Bedingung\n",
    "        df2 =pd.DataFrame([[np.nan, np.nan]], columns=[col_corr, y_train.columns[0]])\n",
    "        modi_df = modi_df.append(df2, ignore_index=True)\n",
    "        \n",
    "        y_files = os.listdir(save_dir + column + '/' + var.y_test + '/' )\n",
    "        results = y_test[[column]].copy()\n",
    "        \n",
    "        for i in y_files:\n",
    "            df_y = pd.read_csv(save_dir + column + '/' + var.y_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            df_x = pd.read_csv(save_dir + column + '/' + var.x_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "             \n",
    "            df_x = df_x.drop(columns=['ID'])  \n",
    "            x_test = transform(df_x, trans_df,True )\n",
    "\n",
    "            for a, v in modi_df.iterrows():\n",
    "                change = x_test[col_corr].ne(v[col_corr])\n",
    "                y_test[column] = y_test[column].where(change,v[column])\n",
    "\n",
    "            results[i[0:-4]] = retransform(y_test, trans_df, types)\n",
    "        results.to_csv( result_dir  + var.methode_4 + '/' + column + '.csv'  ,index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_2_impute(column_set):\n",
    "    for column in column_set:\n",
    "        X_train = pd.read_csv(save_dir + column + '/' + var.x_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_train = pd.read_csv(save_dir + column + '/' + var.y_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_test =  pd.read_csv(save_dir + column + '/' + var.y_test  + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        \n",
    "        df = X_train.copy()\n",
    "        df[y_train.columns] = y_train\n",
    "        df =  df.drop(columns=['ID'])\n",
    "\n",
    "        trans_df = create_trans_table(df)\n",
    "        types = df.dtypes \n",
    "        df_temp = transform(df.copy(), trans_df,True )\n",
    "        #############################################\n",
    "        col_corr = correlation_kendall(df_temp ,y_train.columns[0])\n",
    "        # modi_df DF mit Ziel und Corr Spalte\n",
    "        modi_df = df_temp[[col_corr]] \n",
    "        modi_df[y_train.columns[0]] = df_temp[y_train.columns[0]]\n",
    "        modi_df = modi_df.groupby([col_corr])[y_train.columns[0]].median()\n",
    "        modi_df =  modi_df.reset_index(level=col_corr)\n",
    "        # hinzufügen Missing Bedingung\n",
    "        df2 =pd.DataFrame([[np.nan, np.nan]], columns=[col_corr, y_train.columns[0]])\n",
    "        modi_df = modi_df.append(df2, ignore_index=True)\n",
    "        \n",
    "        y_files = os.listdir(save_dir + column + '/' + var.y_test + '/' )\n",
    "        results = y_test[[column]].copy()\n",
    "        \n",
    "        for i in y_files:\n",
    "            df_y = pd.read_csv(save_dir + column + '/' + var.y_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            df_x = pd.read_csv(save_dir + column + '/' + var.x_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "             \n",
    "            df_x = df_x.drop(columns=['ID'])  \n",
    "            x_test = transform(df_x, trans_df,True )\n",
    "\n",
    "            for a, v in modi_df.iterrows():\n",
    "                change = x_test[col_corr].ne(v[col_corr])\n",
    "                y_test[column] = y_test[column].where(change,v[column])\n",
    "\n",
    "            results[i[0:-4]] = retransform(y_test, trans_df, types)\n",
    "        results.to_csv( result_dir  + var.methode_5 + '/' + column + '.csv'  ,index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_3_impute(column_set):\n",
    "    for column in column_set:\n",
    "        X_train = pd.read_csv(save_dir + column + '/' + var.x_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_train = pd.read_csv(save_dir + column + '/' + var.y_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_test =  pd.read_csv(save_dir + column + '/' + var.y_test  + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        \n",
    "        df = X_train.copy()\n",
    "        df[y_train.columns] = y_train\n",
    "        df =  df.drop(columns=['ID'])\n",
    "\n",
    "        trans_df = create_trans_table(df)\n",
    "        types = df.dtypes \n",
    "        df_temp = transform(df.copy(), trans_df,True )\n",
    "        #############################################\n",
    "        col_corr = correlation_spearman(df_temp ,y_train.columns[0])\n",
    "        # modi_df DF mit Ziel und Corr Spalte\n",
    "        modi_df = df_temp[[col_corr]] \n",
    "        modi_df[y_train.columns[0]] = df_temp[y_train.columns[0]]\n",
    "        modi_df = modi_df.groupby([col_corr])[y_train.columns[0]].median()\n",
    "        modi_df =  modi_df.reset_index(level=col_corr)\n",
    "        # hinzufügen Missing Bedingung\n",
    "        df2 =pd.DataFrame([[np.nan, np.nan]], columns=[col_corr, y_train.columns[0]])\n",
    "        modi_df = modi_df.append(df2, ignore_index=True)\n",
    "        \n",
    "        y_files = os.listdir(save_dir + column + '/' + var.y_test + '/' )\n",
    "        results = y_test[[column]].copy()\n",
    "        \n",
    "        for i in y_files:\n",
    "            df_y = pd.read_csv(save_dir + column + '/' + var.y_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            df_x = pd.read_csv(save_dir + column + '/' + var.x_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "             \n",
    "            df_x = df_x.drop(columns=['ID'])  \n",
    "            x_test = transform(df_x, trans_df,True )\n",
    "\n",
    "            for a, v in modi_df.iterrows():\n",
    "                change = x_test[col_corr].ne(v[col_corr])\n",
    "                y_test[column] = y_test[column].where(change,v[column])\n",
    "                 \n",
    "            results[i[0:-4]] = retransform(y_test, trans_df, types)\n",
    "        results.to_csv( result_dir  + var.methode_6 + '/' + column + '.csv'  ,index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rondom_forest_classification_impute(column_set):\n",
    "    for column in column_set:\n",
    "        X_train = pd.read_csv(save_dir + column + '/' + var.x_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_train = pd.read_csv(save_dir + column + '/' + var.y_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_test =  pd.read_csv(save_dir + column + '/' + var.y_test  + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        X_train = X_train.drop(columns=['ID'])\n",
    "        df = X_train.copy()\n",
    "        df[y_train.columns] = y_train\n",
    "        trans_df = create_trans_table(df)\n",
    "        types = df.dtypes \n",
    "        x_temp = transform(X_train.copy(), trans_df, True )\n",
    "        y_temp = transform(y_train.copy(), trans_df, True )\n",
    "        ##############################################\n",
    "        sc = StandardScaler()\n",
    "        x_temp = sc.fit_transform(x_temp)\n",
    "        x_temp_test = sc.transform(x_temp)\n",
    "        #############################################\n",
    "        best_tree = -1\n",
    "        best_mean_squared_error = -1\n",
    "        ##############################################\n",
    "        for i in [40,50,60,70,80,90,100,110,120,130,140,150,160,170]:\n",
    "            try:\n",
    "                classifier = RandomForestClassifier(n_estimators=i, random_state=0)\n",
    "                classifier.fit(x_temp, y_temp)\n",
    "            #except Exception as exception:\n",
    "            except MemoryError as error:\n",
    "                log_exception(error)\n",
    "                continue\n",
    "            #    print(str(i) + ' kann nicht genommen werden. Speicherporbelme')\n",
    "            ###    continue                \n",
    "            y_pred = classifier.predict(x_temp_test)\n",
    "            ###################################\n",
    "            new_mean_squared_error = metrics.mean_squared_error(y_temp, y_pred)\n",
    "            print(str(i) + \" durch\")\n",
    "            if best_tree == -1 or new_mean_squared_error < best_mean_squared_error:\n",
    "                best_mean_squared_error = new_mean_squared_error\n",
    "                best_tree = i \n",
    "                print(best_tree)\n",
    "                print(best_mean_squared_error)\n",
    "                print(\"Best Tree \" + str(best_tree))\n",
    "        \n",
    "        classifier = RandomForestClassifier(n_estimators=best_tree, random_state=0)\n",
    "        classifier.fit(x_temp, y_temp)\n",
    "        ######################################\n",
    "        ######################################\n",
    "        results = y_test[[column]].copy()\n",
    "        y_files = os.listdir(save_dir + column + '/' + var.y_test + '/' )\n",
    "        for i in y_files:\n",
    "            df_y = pd.read_csv(save_dir + column + '/' + var.y_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            df_x = pd.read_csv(save_dir + column + '/' + var.x_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            \n",
    "            df_x = df_x.drop(columns=['ID'])\n",
    "            x_test = transform(df_x, trans_df, True )\n",
    "            X_test = sc.transform(x_test)\n",
    "            y_pred =  classifier.predict(X_test)\n",
    "            \n",
    "            y_test = pd.DataFrame(data=y_pred, columns=[df_y.columns[0]])\n",
    "            results[i[0:-4]] = retransform(y_test, trans_df, types)\n",
    "        results.to_csv( result_dir  + var.methode_7 + '/' + column + '.csv'  ,index=False,sep=';', decimal=',')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rondom_forest_regression_impute(column_set):\n",
    "    for column in column_set:\n",
    "        X_train = pd.read_csv(save_dir + column + '/' + var.x_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_train = pd.read_csv(save_dir + column + '/' + var.y_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_test =  pd.read_csv(save_dir + column + '/' + var.y_test  + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        \n",
    "        X_train = X_train.drop(columns=['ID'])\n",
    "        trans_df = create_trans_table(X_train)\n",
    "        X_train = transform(X_train.copy(), trans_df, False )\n",
    "        ##############################################\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_train_test = sc.transform(X_train)\n",
    "        #############################################\n",
    "        best_tree = -1\n",
    "        best_mean_squared_error = -1\n",
    "        ##############################################\n",
    "        for i in [40,50,60,70,80,90,100,110,120,130,140,150,160,170]:\n",
    "        #for i in [110]:\n",
    "            ###################################\n",
    "            regressor = RandomForestRegressor(n_estimators=i, random_state=0)\n",
    "            regressor.fit(X_train, y_train)\n",
    "            y_pred = regressor.predict(X_train_test)\n",
    "            ###################################\n",
    "            new_mean_squared_error = metrics.mean_squared_error(y_train, y_pred)\n",
    "            print(str(i) + \" durch\")\n",
    "            if best_tree == -1 or new_mean_squared_error < best_mean_squared_error:\n",
    "                best_mean_squared_error = new_mean_squared_error\n",
    "                best_tree = i \n",
    "                print(best_tree)\n",
    "                print(best_mean_squared_error)\n",
    "                print(\"Best Tree \" + str(best_tree))\n",
    "        \n",
    "        regressor = RandomForestRegressor(n_estimators=best_tree, random_state=0)\n",
    "        regressor.fit(X_train, y_train)\n",
    "        ######################################\n",
    "        ######################################\n",
    "        results = y_test[[column]].copy()\n",
    "        y_files = os.listdir(save_dir + column + '/' + var.y_test + '/' )\n",
    "        for i in y_files:\n",
    "            df_y = pd.read_csv(save_dir + column + '/' + var.y_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            df_x = pd.read_csv(save_dir + column + '/' + var.x_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            \n",
    "            df_x = df_x.drop(columns=['ID'])\n",
    "            df_x = transform(df_x.copy(), trans_df, False )\n",
    "            print(i[0:-4])\n",
    "            X_test = sc.transform(df_x)\n",
    "            y_pred =  regressor.predict(X_test)\n",
    "            results[i[0:-4]] = y_pred\n",
    "        results.to_csv( result_dir  + var.methode_8 + '/' + column + '.csv'  ,index=False,sep=';', decimal=',')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuronal_network_impute(column_set):\n",
    "    for column in column_set:\n",
    "        X_train = pd.read_csv(save_dir + column + '/' + var.x_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_train = pd.read_csv(save_dir + column + '/' + var.y_train + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        y_test =  pd.read_csv(save_dir + column + '/' + var.y_test  + '.csv' , delimiter=\";\", decimal=\",\")\n",
    "        \n",
    "        df_y = pd.read_csv(save_dir + column + '/' + var.y_test + '/' + '0.csv' , delimiter=\";\", decimal=\",\")\n",
    "        df_x = pd.read_csv(save_dir + column + '/' + var.x_test + '/' + '0.csv' , delimiter=\";\", decimal=\",\")\n",
    "        ############\n",
    "        ############\n",
    "        #RANNBATSCHEN\n",
    "        ########\n",
    "        #########\n",
    "        X_train_length =  X_train.shape[0]\n",
    "        X_train = X_train.drop(columns=['ID'])\n",
    "        df_x = df_x.drop(columns=['ID'])\n",
    "        \n",
    "        X_train = X_train.append(df_x, ignore_index=True)\n",
    "        y_train = y_train.append(df_y, ignore_index=True)\n",
    "        X_train[y_train.columns] = y_train\n",
    "        print(X_train.shape)\n",
    "        train_path = save_dir + column + '/train/'  \n",
    "        X_train.to_csv( train_path + 'train' + '.csv' ,index=False,sep=';', decimal=',')\n",
    "        cat_names = ['CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY', 'CNT_CHILDREN', 'NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'FLAG_MOBIL', 'FLAG_WORK_PHONE', 'FLAG_PHONE','FLAG_EMAIL','OCCUPATION_TYPE', 'CNT_FAM_MEMBERS']\n",
    "        cont_names = ['DAYS_BIRTH', 'DAYS_EMPLOYED']\n",
    "        procs = [FillMissing, Categorify, Normalize]\n",
    "        dep_var = column\n",
    "        var_metric = 'mean_absolute_error'\n",
    "        ##############################################\n",
    "        learn = lern_model(train_path,dep_var,cat_names,cont_names, X_train,X_train_length ,X_train.shape[0] ,procs, epoche=1)\n",
    "        ######################################\n",
    "        ######################################\n",
    "        if os.path.exists(statistics_dir + 'statistics.csv'):\n",
    "            statistics = pd.read_csv(statistics_dir + 'statistics.csv' , delimiter=';', decimal=',')\n",
    "        else:\n",
    "            statistics = pd.DataFrame()\n",
    "        columns = np.array(['methode','metric','column', 'misssing', 'Wert'])\n",
    "        metric = 'mean_absolute_error'\n",
    "        y_files = os.listdir(save_dir + column + '/' + var.y_test + '/' )\n",
    "        ###################################\n",
    "        for i in y_files:\n",
    "            df_y = pd.read_csv(save_dir + column + '/' + var.y_test + '/' +  '0.csv' , delimiter=\";\", decimal=\",\")\n",
    "            df_x = pd.read_csv(save_dir + column + '/' + var.x_test + '/' + i , delimiter=\";\", decimal=\",\")\n",
    "            \n",
    "            df_x = df_x.drop(columns=['ID'])\n",
    "            df_test = df_x.copy()\n",
    "            df_test = df_test.fillna(0)\n",
    "            df_test[df_y.columns] = df_y.copy()\n",
    "                                    \n",
    "            data = (TabularList.from_df(df_test, path=train_path, cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "                           .split_by_idx(valid_idx=range(600,1000))\n",
    "                           .label_from_df(cols=dep_var)\n",
    "                           .databunch(bs=64))\n",
    "            print(learn.validate(dl=data.dl(DatasetType.Valid),metrics=[mean_absolute_error])[0])\n",
    "            print('test')\n",
    "            values = np.array([var.methode_9,metric, column])\n",
    "            values = np.append(values, i[0:-4])\n",
    "            values = np.append(values , learn.validate(dl=data.dl(DatasetType.Valid),metrics=[mean_absolute_error])[0])\n",
    "            statistics = pd.DataFrame(np.array([values]), columns=columns).append(statistics, ignore_index=True)\n",
    "        \n",
    "        statistics.to_csv(statistics_dir + 'statistics.csv', index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calc_root_mean_squared_error():\n",
    "    for methode in var.methodes:\n",
    "        print(methode)\n",
    "        \n",
    "        files = os.listdir(result_dir + methode+ '/' )\n",
    "        print(files)\n",
    "        for file in files:\n",
    "            org_column = file[0:-4]\n",
    "            result = pd.read_csv(result_dir + methode+ '/' +  file , delimiter=\";\", decimal=\",\")\n",
    "            if os.path.exists(statistics_dir + 'statistics.csv'):\n",
    "                statistics = pd.read_csv(statistics_dir + 'statistics.csv' , delimiter=\";\", decimal=\",\")\n",
    "            else:\n",
    "                statistics = pd.DataFrame()\n",
    "            \n",
    "            original = result.loc[: , org_column]\n",
    "            result = result.drop(columns=[org_column])\n",
    "            columns = np.array(['methode','type','column', 'misssing', 'Wert'])\n",
    "\n",
    "\n",
    "            for column in result.columns:\n",
    "                print(print(learn.validate(dl=data.dl(DatasetType.Valid),metrics=[mean_absolute_error])))\n",
    "                values = np.array([methode,'root_mean_squared_error', org_column])\n",
    "                values = np.append(values, [column])\n",
    "                values = np.append(values , [np.sqrt(metrics.mean_squared_error(original, result[column]))])\n",
    "                statistics = pd.DataFrame(np.array([values]), columns=columns).append(statistics, ignore_index=True)\n",
    "        \n",
    "\n",
    "            statistics.to_csv(statistics_dir + 'statistics.csv', index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_absolute_error():\n",
    "    for methode in var.methodes:\n",
    "        files = os.listdir(result_dir + methode+ '/' )\n",
    "        for file in files:\n",
    "            org_column = file[0:-4]\n",
    "            result = pd.read_csv(result_dir + methode+ '/' +  file , delimiter=\";\", decimal=\",\")\n",
    "            if os.path.exists(statistics_dir + 'statistics.csv'):\n",
    "                statistics = pd.read_csv(statistics_dir + 'statistics.csv' , delimiter=\";\", decimal=\",\")\n",
    "            else:\n",
    "                statistics = pd.DataFrame()\n",
    "\n",
    "            original = result.loc[: , org_column]\n",
    "            result = result.drop(columns=[org_column])\n",
    "            columns = np.array(['methode','type','column', 'misssing', 'Wert'])\n",
    "\n",
    "        \n",
    "            for column in result.columns:\n",
    "                values = np.array([methode,'mean_absolute_error', org_column])\n",
    "                values = np.append(values, [column])\n",
    "                values = np.append(values , [metrics.mean_absolute_error(original, result[column])])\n",
    "                statistics = pd.DataFrame(np.array([values]), columns=columns).append(statistics, ignore_index=True)\n",
    "\n",
    "            statistics.to_csv(statistics_dir + 'statistics.csv', index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_squared_error():\n",
    "    for methode in var.methodes:\n",
    "        files = os.listdir(result_dir + methode+ '/' )\n",
    "        for file in files:\n",
    "            org_column = file[0:-4]\n",
    "            result = pd.read_csv(result_dir + methode+ '/' +  file , delimiter=\";\", decimal=\",\")\n",
    "            if os.path.exists(statistics_dir + 'statistics.csv'):\n",
    "                statistics = pd.read_csv(statistics_dir + 'statistics.csv' , delimiter=\";\", decimal=\",\")\n",
    "\n",
    "            else:\n",
    "                statistics = pd.DataFrame()\n",
    "            \n",
    "            original = result.loc[: , org_column]\n",
    "            result = result.drop(columns=[org_column])\n",
    "            columns = np.array(['methode','type','column', 'misssing', 'Wert'])\n",
    "\n",
    "        \n",
    "\n",
    "            for column in result.columns:\n",
    "                values = np.array([methode,'mean_squared_error', org_column])\n",
    "                values = np.append(values, [column])\n",
    "                values = np.append(values , [metrics.mean_squared_error(original, result[column])])\n",
    "                statistics = pd.DataFrame(np.array([values]), columns=columns).append(statistics, ignore_index=True)\n",
    "\n",
    "            statistics.to_csv(statistics_dir + 'statistics.csv', index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f1_score():\n",
    "    for methode in var.methodes:\n",
    "        files = os.listdir(result_dir + methode+ '/' )\n",
    "        for file in files:\n",
    "            org_column = file[0:-4]\n",
    "            result = pd.read_csv(result_dir + methode+ '/' +  file , delimiter=\";\", decimal=\",\")\n",
    "            \n",
    "            if os.path.exists(statistics_dir + 'statistics.csv'):\n",
    "                statistics = pd.read_csv(statistics_dir + 'statistics.csv' , delimiter=\";\", decimal=\",\")\n",
    "\n",
    "            else:\n",
    "                statistics = pd.DataFrame()\n",
    "            \n",
    "            original = result.loc[: , org_column]\n",
    "            #if original.dtypes == float:\n",
    "            #    original = original.astype(int)\n",
    "            werte = pd.DataFrame()\n",
    "            for column in result.columns:\n",
    "                werte = werte.append(result[column])\n",
    "            werte = werte.drop_duplicates(ignore_index=True)\n",
    "            trans_df = create_trans_table(werte)\n",
    "            original = transform(original,trans_df,True)\n",
    "            original = original.astype(int)\n",
    "            result = result.drop(columns=[org_column])\n",
    "            columns = np.array(['methode','type','column', 'misssing', 'Wert'])\n",
    "\n",
    "        \n",
    "\n",
    "            for column in result.columns:\n",
    "                \n",
    "                #if result[column].dtypes == float:\n",
    "                #    result[column] = result[column].astype(int)\n",
    "                result[column] = transform(result[column],trans_df,True)\n",
    "                result[column] = result[column].astype(int)\n",
    "                values = np.array([methode,'f1_score', org_column])\n",
    "                values = np.append(values, [column])\n",
    "                values = np.append(values , [metrics.f1_score(original, result[column], average='weighted',labels=np.unique(result[column]))])\n",
    "                statistics = pd.DataFrame(np.array([values]), columns=columns).append(statistics, ignore_index=True)\n",
    "\n",
    "            statistics.to_csv(statistics_dir + 'statistics.csv', index=False,sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split_data(csv_file,filename, column, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampute(var.application_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " means_impute(var.application_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_impute(var.application_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_impute(var.application_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "correlation_1_impute(var.application_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "correlation_2_impute(var.application_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GreenBook\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "correlation_3_impute(var.application_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rondom_forest_regression_impute(var.application_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rondom_forest_classification_impute(var.application_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation_1\n",
      "['AMT_INCOME_TOTAL.csv']\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_abs_correlations(df, n=5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(304354, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>root_mean_squared_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>52011986944.000000</td>\n",
       "      <td>51122937856.000000</td>\n",
       "      <td>225396.546875</td>\n",
       "      <td>01:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/7 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-d273cfe97fa8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mneuronal_network_impute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplication_record\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-afd1b47a74aa>\u001b[0m in \u001b[0;36mneuronal_network_impute\u001b[1;34m(column_set)\u001b[0m\n\u001b[0;32m     52\u001b[0m                            \u001b[1;33m.\u001b[0m\u001b[0mlabel_from_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdep_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                            .databunch(bs=64))\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mValid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethode_9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(self, dl, callbacks, metrics)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[0mcb_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCallbackHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mifnone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[0mval_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m         \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_metrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'last_metrics'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(model, dl, loss_func, cb_handler, pbar, average, n_batch)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpbar\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb_handler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m             \u001b[0mval_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0myb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[1;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mxb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0myb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\tabular\\models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x_cat, x_cont)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_cat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_cont\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_emb\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memb_drop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\tabular\\models.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_cat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_cont\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_emb\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memb_drop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m         return F.embedding(\n\u001b[0;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1812\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1814\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "neuronal_network_impute(var.application_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
